{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T19:43:52.006881Z",
     "start_time": "2020-05-02T19:43:52.004297Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T16:15:57.296564Z",
     "start_time": "2020-05-30T16:15:57.292197Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.tag.stanford import StanfordPOSTagger as POS_Tag \n",
    "# Download the tagger from the site: https://nlp.stanford.edu/software/tagger.shtml Extract the file:stanford-postagger.jar\n",
    "from nltk import FreqDist\n",
    "import random\n",
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read clean data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_twitts = pd.read_csv('data/clean/es_twitts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Tokenizing and Removing Noise from the Data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_noise(desc, stop_words = ()):        \n",
    "        \n",
    "    desc = re.sub('https?:\\/\\/.*[\\r\\n]*','', desc)    \n",
    "    desc = re.sub(\"[^A-ZÑa-zñ]+\",\" \", desc)\n",
    "    desc = desc.lower()\n",
    "    cleaned_tokens = []    \n",
    "    \n",
    "    for token in desc.split(): ## Tokenizing the Data        \n",
    "        if token not in stop_words:\n",
    "            #print(token)\n",
    "            cleaned_tokens.append(token)\n",
    "        #else:\n",
    "            #print('STOPWORDS:',token)\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gibra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load stopwords\n",
    "stop_words = stopwords.words('spanish')\n",
    "stop_words.extend(['mexico', 'coronavirusoutbreak', 'casos', 'covid', 'coronavirus', 'coronavid', 'coronaviruspandemic', 'pandemia', 'cuarentena', 'virus','cdmx','mx'])\n",
    "\n",
    "# TODO: Use pandas table \n",
    "tweets = es_twitts['text'].tolist()\n",
    "tweets_cleaned_tokens_list = []\n",
    "\n",
    "for tokens in tweets:\n",
    "    # print(tokens)\n",
    "    tweets_cleaned_tokens_list.append(remove_noise(tokens, stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Si empezaste a trabajar, necesitas dar de alta a tus beneficiarios ante el IMSS, ahora lo puedes hacer desde tu domicilio a traves de internet y evita filas | SanaDistancia\n",
      "QuedateEnCasa \n",
      "Coronavirus\n",
      "COVID19 MexicoUnido          \n",
      "\n",
      "https://t.co/zv3POwhVXe https://t.co/1VOKagjdOF\n",
      "\n",
      "['Si', 'empezaste', 'a', 'trabajar,', 'necesitas', 'dar', 'de', 'alta', 'a', 'tus', 'beneficiarios', 'ante', 'el', 'IMSS,', 'ahora', 'lo', 'puedes', 'hacer', 'desde', 'tu', 'domicilio', 'a', 'traves', 'de', 'internet', 'y', 'evita', 'filas', '|', 'SanaDistancia', 'QuedateEnCasa', 'Coronavirus', 'COVID19', 'MexicoUnido', 'https://t.co/zv3POwhVXe', 'https://t.co/1VOKagjdOF']\n",
      "\n",
      "['si', 'empezaste', 'trabajar', 'necesitas', 'dar', 'alta', 'beneficiarios', 'imss', 'ahora', 'puedes', 'hacer', 'domicilio', 'traves', 'internet', 'evita', 'filas', 'sanadistancia', 'quedateencasa', 'mexicounido']\n"
     ]
    }
   ],
   "source": [
    "x=2\n",
    "print(tweets[x])\n",
    "print('')\n",
    "print(tweets[x].split())\n",
    "print('')\n",
    "print(tweets_cleaned_tokens_list[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Normalizing data   from the Data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish_postagger = POS_Tag(\n",
    "    r'stanford-tagger\\models\\spanish-ud.tagger',\n",
    "    r'stanford-tagger\\stanford-postagger-4.0.0.jar',\n",
    "    encoding='utf8'\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(words,taggstarts):\n",
    "    nouns = []    \n",
    "    tagged_words = spanish_postagger.tag(words) ### Normalizing data            \n",
    "    print (tagged_words) \n",
    "    \n",
    "    for (word, tag) in tagged_words:      \n",
    "        print(word + ' ' + tag)        \n",
    "        if tag.startswith(taggstarts):\n",
    "            nouns.append(word)            \n",
    "    #print(nouns)    \n",
    "    return nouns\n",
    "\n",
    "#https://stackoverflow.com/questions/14732465/nltk-tagging-spanish-words-using-a-corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cualquier', 'DET'), ('enfermedad', 'NOUN'), ('respiratoria', 'ADJ'), ('automediques', 'ADJ'), ('prevencioncoronavirus', 'ADJ')]\n",
      "cualquier DET\n",
      "enfermedad NOUN\n",
      "respiratoria ADJ\n",
      "automediques ADJ\n",
      "prevencioncoronavirus ADJ\n",
      "[('cualquier', 'DET'), ('enfermedad', 'NOUN'), ('respiratoria', 'ADJ'), ('automediques', 'ADJ'), ('prevencioncoronavirus', 'ADJ')]\n",
      "cualquier DET\n",
      "enfermedad NOUN\n",
      "respiratoria ADJ\n",
      "automediques ADJ\n",
      "prevencioncoronavirus ADJ\n",
      "[('cualquier', 'DET'), ('enfermedad', 'NOUN'), ('respiratoria', 'ADJ'), ('automediques', 'ADJ'), ('prevencioncoronavirus', 'ADJ')]\n",
      "cualquier DET\n",
      "enfermedad NOUN\n",
      "respiratoria ADJ\n",
      "automediques ADJ\n",
      "prevencioncoronavirus ADJ\n",
      "[('atenci', 'NOUN'), ('n', 'NOUN'), ('terminal', 'NOUN'), ('nuevo', 'ADJ'), ('circo', 'NOUN'), ('implementan', 'VERB'), ('medidas', 'NOUN'), ('uso', 'NOUN'), ('mascarilla', 'VERB'), ('parte', 'NOUN'), ('usuaris', 'NOUN'), ('conductores', 'ADJ'), ('hacen', 'VERB'), ('vida', 'NOUN'), ('dichas', 'ADJ'), ('instalaciones', 'NOUN'), ('parte', 'NOUN'), ('esfuerzos', 'NOUN'), ('unificados', 'ADJ'), ('impedir', 'AUX'), ('propagacion', 'VERB'), ('marzo', 'NOUN'), ('nicolasmaduro', 'ADJ'), ('erikapsuv', 'ADJ')]\n",
      "atenci NOUN\n",
      "n NOUN\n",
      "terminal NOUN\n",
      "nuevo ADJ\n",
      "circo NOUN\n",
      "implementan VERB\n",
      "medidas NOUN\n",
      "uso NOUN\n",
      "mascarilla VERB\n",
      "parte NOUN\n",
      "usuaris NOUN\n",
      "conductores ADJ\n",
      "hacen VERB\n",
      "vida NOUN\n",
      "dichas ADJ\n",
      "instalaciones NOUN\n",
      "parte NOUN\n",
      "esfuerzos NOUN\n",
      "unificados ADJ\n",
      "impedir AUX\n",
      "propagacion VERB\n",
      "marzo NOUN\n",
      "nicolasmaduro ADJ\n",
      "erikapsuv ADJ\n",
      "[('atenci', 'NOUN'), ('n', 'NOUN'), ('terminal', 'NOUN'), ('nuevo', 'ADJ'), ('circo', 'NOUN'), ('implementan', 'VERB'), ('medidas', 'NOUN'), ('uso', 'NOUN'), ('mascarilla', 'VERB'), ('parte', 'NOUN'), ('usuaris', 'NOUN'), ('conductores', 'ADJ'), ('hacen', 'VERB'), ('vida', 'NOUN'), ('dichas', 'ADJ'), ('instalaciones', 'NOUN'), ('parte', 'NOUN'), ('esfuerzos', 'NOUN'), ('unificados', 'ADJ'), ('impedir', 'AUX'), ('propagacion', 'VERB'), ('marzo', 'NOUN'), ('nicolasmaduro', 'ADJ'), ('erikapsuv', 'ADJ')]\n",
      "atenci NOUN\n",
      "n NOUN\n",
      "terminal NOUN\n",
      "nuevo ADJ\n",
      "circo NOUN\n",
      "implementan VERB\n",
      "medidas NOUN\n",
      "uso NOUN\n",
      "mascarilla VERB\n",
      "parte NOUN\n",
      "usuaris NOUN\n",
      "conductores ADJ\n",
      "hacen VERB\n",
      "vida NOUN\n",
      "dichas ADJ\n",
      "instalaciones NOUN\n",
      "parte NOUN\n",
      "esfuerzos NOUN\n",
      "unificados ADJ\n",
      "impedir AUX\n",
      "propagacion VERB\n",
      "marzo NOUN\n",
      "nicolasmaduro ADJ\n",
      "erikapsuv ADJ\n",
      "[('atenci', 'NOUN'), ('n', 'NOUN'), ('terminal', 'NOUN'), ('nuevo', 'ADJ'), ('circo', 'NOUN'), ('implementan', 'VERB'), ('medidas', 'NOUN'), ('uso', 'NOUN'), ('mascarilla', 'VERB'), ('parte', 'NOUN'), ('usuaris', 'NOUN'), ('conductores', 'ADJ'), ('hacen', 'VERB'), ('vida', 'NOUN'), ('dichas', 'ADJ'), ('instalaciones', 'NOUN'), ('parte', 'NOUN'), ('esfuerzos', 'NOUN'), ('unificados', 'ADJ'), ('impedir', 'AUX'), ('propagacion', 'VERB'), ('marzo', 'NOUN'), ('nicolasmaduro', 'ADJ'), ('erikapsuv', 'ADJ')]\n",
      "atenci NOUN\n",
      "n NOUN\n",
      "terminal NOUN\n",
      "nuevo ADJ\n",
      "circo NOUN\n",
      "implementan VERB\n",
      "medidas NOUN\n",
      "uso NOUN\n",
      "mascarilla VERB\n",
      "parte NOUN\n",
      "usuaris NOUN\n",
      "conductores ADJ\n",
      "hacen VERB\n",
      "vida NOUN\n",
      "dichas ADJ\n",
      "instalaciones NOUN\n",
      "parte NOUN\n",
      "esfuerzos NOUN\n",
      "unificados ADJ\n",
      "impedir AUX\n",
      "propagacion VERB\n",
      "marzo NOUN\n",
      "nicolasmaduro ADJ\n",
      "erikapsuv ADJ\n"
     ]
    }
   ],
   "source": [
    "n_tweets_noun_tokens_list = []\n",
    "a_tweets_noun_tokens_list = []\n",
    "v_tweets_noun_tokens_list = []\n",
    "for words in tweets_cleaned_tokens_list[:2]:    \n",
    "    n_tweets_noun_tokens_list.append(normalization(words,'N'))\n",
    "    a_tweets_noun_tokens_list.append(normalization(words,'A'))\n",
    "    v_tweets_noun_tokens_list.append(normalization(words,'V'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['enfermedad'],\n",
       " ['atenci',\n",
       "  'n',\n",
       "  'terminal',\n",
       "  'circo',\n",
       "  'medidas',\n",
       "  'uso',\n",
       "  'parte',\n",
       "  'usuaris',\n",
       "  'vida',\n",
       "  'instalaciones',\n",
       "  'parte',\n",
       "  'esfuerzos',\n",
       "  'marzo']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_tweets_noun_tokens_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['enfermedad']\n",
      "\n",
      "['enfermedad']\n"
     ]
    }
   ],
   "source": [
    "x=0\n",
    "print(n_tweets_noun_tokens_list[x])\n",
    "print ('')\n",
    "print(n_tweets_noun_tokens_list[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Determining Word Density</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_words(cleaned_tokens_list):\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        for token in tokens:\n",
    "            yield token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('parte', 2), ('enfermedad', 1), ('atenci', 1), ('n', 1), ('terminal', 1), ('circo', 1), ('medidas', 1), ('uso', 1), ('usuaris', 1), ('vida', 1), ('instalaciones', 1), ('esfuerzos', 1), ('marzo', 1)]\n"
     ]
    }
   ],
   "source": [
    "#all_pos_words = get_all_words(tweets_cleaned_tokens_list)\n",
    "all_pos_words = get_all_words(n_tweets_noun_tokens_list)\n",
    "freq_dist_pos = FreqDist(all_pos_words)\n",
    "print(freq_dist_pos.most_common(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Preparing Data for the Model</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting Tokens to a Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets_for_model(cleaned_tokens_list):\n",
    "    for tweet_tokens in cleaned_tokens_list:\n",
    "        yield dict([token, True] for token in tweet_tokens)\n",
    "\n",
    "positive_tokens_for_model = get_tweets_for_model(n_tweets_noun_tokens_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Splitting the Dataset for Training and Testing the Model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.3 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-2dc81b51faef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mtrain_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mtrain_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m7000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[1;34m(*arrays, **options)\u001b[0m\n\u001b[0;32m   2098\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2099\u001b[0m     n_train, n_test = _validate_shuffle_split(n_samples, test_size, train_size,\n\u001b[1;32m-> 2100\u001b[1;33m                                               default_test_size=0.25)\n\u001b[0m\u001b[0;32m   2101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2102\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36m_validate_shuffle_split\u001b[1;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[0;32m   1780\u001b[0m             \u001b[1;34m'resulting train set will be empty. Adjust any of the '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1781\u001b[0m             'aforementioned parameters.'.format(n_samples, test_size,\n\u001b[1;32m-> 1782\u001b[1;33m                                                 train_size)\n\u001b[0m\u001b[0;32m   1783\u001b[0m         )\n\u001b[0;32m   1784\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: With n_samples=0, test_size=0.3 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "positive_dataset = [(tweet_dict, \"Positive\")\n",
    "                     for tweet_dict in positive_tokens_for_model]\n",
    "\n",
    "dataset = positive_dataset\n",
    "\n",
    "random.shuffle(dataset)\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# train_set, test_set = train_test_split(dataset, test_size=0.30, random_state=42)\n",
    "\n",
    "train_data = dataset[:7000]\n",
    "test_data = dataset[7000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.3 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-32421d967227>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[1;34m(*arrays, **options)\u001b[0m\n\u001b[0;32m   2098\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2099\u001b[0m     n_train, n_test = _validate_shuffle_split(n_samples, test_size, train_size,\n\u001b[1;32m-> 2100\u001b[1;33m                                               default_test_size=0.25)\n\u001b[0m\u001b[0;32m   2101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2102\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36m_validate_shuffle_split\u001b[1;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[0;32m   1780\u001b[0m             \u001b[1;34m'resulting train set will be empty. Adjust any of the '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1781\u001b[0m             'aforementioned parameters.'.format(n_samples, test_size,\n\u001b[1;32m-> 1782\u001b[1;33m                                                 train_size)\n\u001b[0m\u001b[0;32m   1783\u001b[0m         )\n\u001b[0;32m   1784\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: With n_samples=0, test_size=0.3 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "train_test_split(dataset, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'atenci': True,\n",
       "   'n': True,\n",
       "   'terminal': True,\n",
       "   'circo': True,\n",
       "   'medidas': True,\n",
       "   'uso': True,\n",
       "   'parte': True,\n",
       "   'usuaris': True,\n",
       "   'vida': True,\n",
       "   'instalaciones': True,\n",
       "   'esfuerzos': True,\n",
       "   'marzo': True},\n",
       "  'Positive'),\n",
       " ({'enfermedad': True}, 'Positive')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Building and Testing the Model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0\n",
      "Most Informative Features\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "classifier = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n",
    "\n",
    "print(classifier.show_most_informative_features(10))"
   ]
  }
 ],
 "metadata": {
  "hide_input": true,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "209.8px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 531.8,
   "position": {
    "height": "553.4px",
    "left": "930px",
    "right": "20px",
    "top": "120px",
    "width": "587.8px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
