{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On this phase we will train and test the sentiment analysis model using NLP (Natural Lenguage Processing) which involves classifying tweet-texts in a pre-defined sentiment. Important mention, to train the model the dataset used will be from corpus available here :\n",
    "\n",
    "http://tass.sepln.org/tass_data/download.php?auth=4tNaxs9su4VeTvJejrj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xml.etree.ElementTree as etree\n",
    "import os\n",
    "from glob import glob\n",
    "from nltk.corpus import stopwords\n",
    "import re, string\n",
    "from nltk.tag.stanford import StanfordPOSTagger as POS_Tag\n",
    "from nltk import FreqDist\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import twitts classified to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_classified = pd.DataFrame()\n",
    "columns = [\"content\",\"sentiment\"]\n",
    "df_tweets_classified = pd.DataFrame(columns = columns)\n",
    "\n",
    "raw_data_path = r'data/raw/*TASS2019*.xml'\n",
    "xml_files = glob(raw_data_path)\n",
    "xml_files\n",
    "\n",
    "for files in xml_files:\n",
    "    tree = etree.parse(files)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    for node in root: \n",
    "        tweet = node.attrib.get(\"tweet\")    \n",
    "        content = node.find(\"content\").text if node is not None else None\n",
    "        sentiment = node.find(\"sentiment/polarity/value\").text if node is not None else None        \n",
    "        df_tweets_classified = df_tweets_classified.append(pd.Series([content,sentiment], index = columns), ignore_index = True)    \n",
    "\n",
    "df_tweets_classified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Noise from the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .words() method to get a list of stop words in Spanish\n",
    "stop_words = stopwords.words('spanish')\n",
    "\n",
    "# word accents from stopword list are removed for the clean process\n",
    "stop_words = [re.sub('á','a', i) for i in stop_words]\n",
    "stop_words = [re.sub('é','e', i) for i in stop_words]\n",
    "stop_words = [re.sub('í','i', i) for i in stop_words]\n",
    "stop_words = [re.sub('ó','o', i) for i in stop_words]\n",
    "stop_words = [re.sub('ú','u', i) for i in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform all words in lower case in a new column called \"clean_text\"\n",
    "df_tweets_classified[\"clean_content\"] = df_tweets_classified[\"content\"].str.lower()\n",
    "\n",
    "# word accents from tweet list are removed for the clean process\n",
    "df_tweets_classified.replace('á','a', regex=True, inplace=True)\n",
    "df_tweets_classified.replace('é','e', regex=True, inplace=True)\n",
    "df_tweets_classified.replace('í','i', regex=True, inplace=True)\n",
    "df_tweets_classified.replace('ó','o', regex=True, inplace=True)\n",
    "df_tweets_classified.replace('ú','u', regex=True, inplace=True)\n",
    "\n",
    "# remove from words urls, tags, hastashs, special characters and words which contain 1 to 3 letters.  \n",
    "df_tweets_classified['clean_content'].replace('http\\S+','',regex=True, inplace = True)\n",
    "df_tweets_classified['clean_content'] = df_tweets_classified['clean_content'].map(lambda x: re.sub(r'@\\S+', ' ', x))\n",
    "df_tweets_classified['clean_content'] = df_tweets_classified['clean_content'].map(lambda x: re.sub(r'#\\S+', ' ', x))\n",
    "df_tweets_classified['clean_content'] = df_tweets_classified['clean_content'].map(lambda x: re.sub(r'\\b\\w{1,3}\\b', ' ', x))\n",
    "df_tweets_classified['clean_content'] = df_tweets_classified['clean_content'].map(lambda x: re.sub(r'[^a-zñ]+', ' ', x))\n",
    "\n",
    "# Set \"NaN\" to create a NaN value for those empty fields after cleaning process\n",
    "df_tweets_classified.replace(\"\", float(\"NaN\"), inplace=True)\n",
    "\n",
    "# drop all rows that contain NaN under conten clean column,this for ensuring not letting blank values .\n",
    "df_tweets_classified.dropna(subset = [\"clean_content\"], inplace=True)\n",
    "\n",
    "df_tweets_classified['clean_content']=df_tweets_classified['clean_content'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "df_tweets_classified[['content','clean_content']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Data by Positive and Negative Sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_classified.sentiment.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweets = df_tweets_classified.query(\"sentiment == 'P'\")\n",
    "positive_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_tweets = df_tweets_classified.query(\"sentiment == 'N'\")\n",
    "negative_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing is by splitting the text based on whitespace and punctuation, token is a sequence of characters obtained by text that serves as a unit, basically we will create words from text, it will help make easier for the understanding machine process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_token_tweets_list=[]\n",
    "neg_token_tweets_list=[]\n",
    "\n",
    "for token in positive_tweets['clean_content'].str.split():   \n",
    "    pos_token_tweets_list.append(token)\n",
    "\n",
    "for token in negative_tweets['clean_content'].str.split():          \n",
    "    neg_token_tweets_list.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pos_token_tweets_list[9],'\\n')\n",
    "print(neg_token_tweets_list[9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing the Data - Fast way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing process takes some time of execution, you could skip the following \"Manual way\" and use this \"Fast way\", just running below block code which imports the tokens already normalized. If you want understand the process of normalization skip this and go for normalazing data - manual way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Tokens already normalized\n",
    "pos_tokens_normalized = []\n",
    "neg_tokens_normalized = []\n",
    "\n",
    "# Import\n",
    "with open(r'data\\clean\\pos_tokens_normalized.txt', \"r\") as f:\n",
    "    for line in f:\n",
    "        pos_tokens_normalized.append(line.split())        \n",
    "\n",
    "with open(r'data\\clean\\neg_tokens_normalized.txt', \"r\") as f:\n",
    "    for line in f:\n",
    "        neg_tokens_normalized.append(line.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing the Data - Manual way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Normalization \"Manual way\" ensure have downloaded the JAVA JRE since due space limitation on GitHub it's not not included, download the version jre1.8.0_251 from below link and place it at the root path of this project.\n",
    "\n",
    "https://www.oracle.com/java/technologies/javase-jre8-downloads.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish_postagger = POS_Tag(r'stanford-tagger/models/spanish.tagger', r'stanford-tagger/stanford-postagger.jar', encoding='utf8')\n",
    "java_path = \"jre1.8.0_251/bin/java.exe\"\n",
    "os.environ['JAVAHOME'] = java_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(words):    \n",
    "    tokens = []\n",
    "    tagged_words = spanish_postagger.tag(words) ### Normalizing data            \n",
    "    #print (tagged_words) \n",
    "        \n",
    "    for (word, tag) in tagged_words:                      \n",
    "        if tag not in ['np00000','word','nc0n000','di0000','pr000000','vaip000','sp000','z0','i']:\n",
    "            #print(word+' '+tag)\n",
    "            tokens.append(word)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tokens_normalized = []\n",
    "neg_tokens_normalized = []\n",
    "\n",
    "for words in pos_token_tweets_list[:1]:\n",
    "    pos_tokens_normalized.append(normalization(words))\n",
    "\n",
    "for words in neg_token_tweets_list[:1]:\n",
    "    neg_tokens_normalized.append(normalization(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation normal tweets vs clean tweets, tokenized and normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x=130\n",
    "x=0\n",
    "print(positive_tweets.content.iloc[x])\n",
    "print(pos_tokens_normalized[x],'\\n')\n",
    "\n",
    "print(negative_tweets.content.iloc[x])\n",
    "print(neg_tokens_normalized[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining Word Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_words(cleaned_tokens_list):\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        for token in tokens:\n",
    "            yield token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pos_words = get_all_words(pos_tokens_normalized)\n",
    "all_neg_words = get_all_words(neg_tokens_normalized)\n",
    "\n",
    "freq_dist_pos = FreqDist(all_pos_words)\n",
    "freq_dist_neg = FreqDist(all_neg_words)\n",
    "\n",
    "print(freq_dist_pos.most_common(20),'\\n')\n",
    "print(freq_dist_neg.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data for the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting Tokens to a Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets_for_model(cleaned_tokens_list):\n",
    "    for tweet_tokens in cleaned_tokens_list:\n",
    "        yield dict([token, True] for token in tweet_tokens)\n",
    "\n",
    "positive_tokens_for_model = get_tweets_for_model(pos_tokens_normalized)\n",
    "negative_tokens_for_model = get_tweets_for_model(neg_tokens_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_count=len(pos_tokens_normalized)\n",
    "neg_count=len(neg_tokens_normalized)\n",
    "print(pos_count,neg_count,'=',pos_count+neg_count )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the Dataset for Training and Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "positive_dataset = [(tweet_dict, \"Positive\")\n",
    "                     for tweet_dict in positive_tokens_for_model]\n",
    "\n",
    "negative_dataset = [(tweet_dict, \"Negative\")\n",
    "                     for tweet_dict in negative_tokens_for_model]\n",
    "\n",
    "dataset = positive_dataset + negative_dataset\n",
    "\n",
    "random.shuffle(dataset)\n",
    "\n",
    "train_data = dataset[:2500]\n",
    "test_data = dataset[2500:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "classifier = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n",
    "\n",
    "print(classifier.show_most_informative_features(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model with custom messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_tweet = \"que malos dias ahorita con esto del covid\"\n",
    "\n",
    "custom_tokens = word_tokenize(custom_tweet)\n",
    "\n",
    "print(classifier.classify(dict([token, True] for token in custom_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Model trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensorflow\n",
    "# To save:\n",
    "import pickle\n",
    "f = open('sentiment_classifier.pickle', 'wb')\n",
    "pickle.dump(classifier, f)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
