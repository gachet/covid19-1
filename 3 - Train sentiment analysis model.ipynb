{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xml.etree.ElementTree as etree\n",
    "import os\n",
    "from glob import glob\n",
    "from nltk.corpus import stopwords\n",
    "import re, string\n",
    "from nltk.tag.stanford import StanfordPOSTagger as POS_Tag #Download the tagger from the site: https://nlp.stanford.edu/software/tagger.shtml Extract the file:stanford-postagger.jar\n",
    "from nltk import FreqDist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import twitts classified to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://tass.sepln.org/tass_data/download.php?auth=4tNaxs9su4VeTvJejrj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>@NoilyMV yo soy totalmente puntual</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>@SandraCauffman Hola Sandrita. No le habia des...</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Si andan haciendo eso mejor se quedaran callad...</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Que pereza quiero choco banano</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>@robertobrenes Bueno, no es tanto lo mayor com...</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4795</td>\n",
       "      <td>@AmorAKilates @Roocio_Mk si me pasa lo mismo!</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4796</td>\n",
       "      <td>@clauchoarrionda pquno ladra y ls demas retwitean</td>\n",
       "      <td>NEU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4797</td>\n",
       "      <td>A mi desayuno le hizo falta un alfajor podrida...</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4798</td>\n",
       "      <td>Viste cuando necesitas que alguien te escuche ...</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4799</td>\n",
       "      <td>@cuervotinelli @candetinelli Mañana es mi prim...</td>\n",
       "      <td>NEU</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4800 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                content sentiment\n",
       "0                    @NoilyMV yo soy totalmente puntual      NONE\n",
       "1     @SandraCauffman Hola Sandrita. No le habia des...         P\n",
       "2     Si andan haciendo eso mejor se quedaran callad...         N\n",
       "3                        Que pereza quiero choco banano         N\n",
       "4     @robertobrenes Bueno, no es tanto lo mayor com...         N\n",
       "...                                                 ...       ...\n",
       "4795      @AmorAKilates @Roocio_Mk si me pasa lo mismo!      NONE\n",
       "4796  @clauchoarrionda pquno ladra y ls demas retwitean       NEU\n",
       "4797  A mi desayuno le hizo falta un alfajor podrida...         N\n",
       "4798  Viste cuando necesitas que alguien te escuche ...         N\n",
       "4799  @cuervotinelli @candetinelli Mañana es mi prim...       NEU\n",
       "\n",
       "[4800 rows x 2 columns]"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets_classified = pd.DataFrame()\n",
    "columns = [\"content\",\"sentiment\"]\n",
    "df_tweets_classified = pd.DataFrame(columns = columns)\n",
    "\n",
    "raw_data_path = r'data/raw/*TASS2019*.xml'\n",
    "xml_files = glob(raw_data_path)\n",
    "xml_files\n",
    "\n",
    "for files in xml_files:\n",
    "    tree = etree.parse(files)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    for node in root: \n",
    "        tweet = node.attrib.get(\"tweet\")    \n",
    "        content = node.find(\"content\").text if node is not None else None\n",
    "        sentiment = node.find(\"sentiment/polarity/value\").text if node is not None else None        \n",
    "        df_tweets_classified = df_tweets_classified.append(pd.Series([content,sentiment], index = columns), ignore_index = True)    \n",
    "\n",
    "df_tweets_classified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Noise from the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('spanish')\n",
    "#stop_words.extend([''])\n",
    "\n",
    "stop_words = [re.sub('á','a', i) for i in stop_words]\n",
    "stop_words = [re.sub('é','e', i) for i in stop_words]\n",
    "stop_words = [re.sub('í','i', i) for i in stop_words]\n",
    "stop_words = [re.sub('ó','o', i) for i in stop_words]\n",
    "stop_words = [re.sub('ú','u', i) for i in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>clean_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>@NoilyMV yo soy totalmente puntual</td>\n",
       "      <td>totalmente puntual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>@SandraCauffman Hola Sandrita. No le habia des...</td>\n",
       "      <td>hola sandrita deseado feliz dia madre tarde se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Si andan haciendo eso mejor se quedaran callad...</td>\n",
       "      <td>andan haciendo mejor quedaran calladitas jaja ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Que pereza quiero choco banano</td>\n",
       "      <td>pereza quiero choco banano</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>@robertobrenes Bueno, no es tanto lo mayor com...</td>\n",
       "      <td>bueno mayor cuanto campo usted sos cartaguito ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4795</td>\n",
       "      <td>@AmorAKilates @Roocio_Mk si me pasa lo mismo!</td>\n",
       "      <td>pasa mismo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4796</td>\n",
       "      <td>@clauchoarrionda pquno ladra y ls demas retwitean</td>\n",
       "      <td>pquno ladra demas retwitean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4797</td>\n",
       "      <td>A mi desayuno le hizo falta un alfajor podrida...</td>\n",
       "      <td>desayuno hizo falta alfajor podrida galletas a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4798</td>\n",
       "      <td>Viste cuando necesitas que alguien te escuche ...</td>\n",
       "      <td>viste necesitas alguien escuche alguien bueno</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4799</td>\n",
       "      <td>@cuervotinelli @candetinelli Mañana es mi prim...</td>\n",
       "      <td>mañana primer sesion vamo tapar tajo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4800 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                content  \\\n",
       "0                    @NoilyMV yo soy totalmente puntual   \n",
       "1     @SandraCauffman Hola Sandrita. No le habia des...   \n",
       "2     Si andan haciendo eso mejor se quedaran callad...   \n",
       "3                        Que pereza quiero choco banano   \n",
       "4     @robertobrenes Bueno, no es tanto lo mayor com...   \n",
       "...                                                 ...   \n",
       "4795      @AmorAKilates @Roocio_Mk si me pasa lo mismo!   \n",
       "4796  @clauchoarrionda pquno ladra y ls demas retwitean   \n",
       "4797  A mi desayuno le hizo falta un alfajor podrida...   \n",
       "4798  Viste cuando necesitas que alguien te escuche ...   \n",
       "4799  @cuervotinelli @candetinelli Mañana es mi prim...   \n",
       "\n",
       "                                          clean_content  \n",
       "0                                    totalmente puntual  \n",
       "1     hola sandrita deseado feliz dia madre tarde se...  \n",
       "2     andan haciendo mejor quedaran calladitas jaja ...  \n",
       "3                            pereza quiero choco banano  \n",
       "4     bueno mayor cuanto campo usted sos cartaguito ...  \n",
       "...                                                 ...  \n",
       "4795                                         pasa mismo  \n",
       "4796                        pquno ladra demas retwitean  \n",
       "4797  desayuno hizo falta alfajor podrida galletas a...  \n",
       "4798      viste necesitas alguien escuche alguien bueno  \n",
       "4799               mañana primer sesion vamo tapar tajo  \n",
       "\n",
       "[4800 rows x 2 columns]"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets_classified[\"clean_content\"] = df_tweets_classified[\"content\"].str.lower()\n",
    "\n",
    "# change accentsb\n",
    "df_tweets_classified.replace('á','a', regex=True, inplace=True)\n",
    "df_tweets_classified.replace('é','e', regex=True, inplace=True)\n",
    "df_tweets_classified.replace('í','i', regex=True, inplace=True)\n",
    "df_tweets_classified.replace('ó','o', regex=True, inplace=True)\n",
    "df_tweets_classified.replace('ú','u', regex=True, inplace=True)\n",
    "\n",
    "df_tweets_classified['clean_content'].replace('http\\S+','',regex=True, inplace = True)\n",
    "df_tweets_classified['clean_content'] = df_tweets_classified['clean_content'].map(lambda x: re.sub(r'@\\S+', ' ', x))\n",
    "df_tweets_classified['clean_content'] = df_tweets_classified['clean_content'].map(lambda x: re.sub(r'#\\S+', ' ', x))\n",
    "df_tweets_classified['clean_content'] = df_tweets_classified['clean_content'].map(lambda x: re.sub(r'\\b\\w{1}\\s', ' ', x))\n",
    "df_tweets_classified['clean_content'] = df_tweets_classified['clean_content'].map(lambda x: re.sub(r'\\b\\w{2}\\s', ' ', x))\n",
    "df_tweets_classified['clean_content'] = df_tweets_classified['clean_content'].map(lambda x: re.sub(r'[^a-zñ]+', ' ', x))\n",
    "\n",
    "df_tweets_classified['clean_content']=df_tweets_classified['clean_content'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "df_tweets_classified[['content','clean_content']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Data by Positive and Negative Sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NONE', 'P', 'N', 'NEU'], dtype=object)"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets_classified.sentiment.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>clean_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>@SandraCauffman Hola Sandrita. No le habia des...</td>\n",
       "      <td>P</td>\n",
       "      <td>hola sandrita deseado feliz dia madre tarde se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>@doriamdiaz El de Halfon de Germinal se ve mor...</td>\n",
       "      <td>P</td>\n",
       "      <td>halfon germinal mortal mary bang volando todav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>El amor es paciente, es bondadoso, no es envid...</td>\n",
       "      <td>P</td>\n",
       "      <td>amor paciente bondadoso envidioso orgulloso eg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>El amanecer respirando o2 puro, es mas que un ...</td>\n",
       "      <td>P</td>\n",
       "      <td>amanecer respirando puro regalo frente mar pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>Buenas noches papus, que descansen</td>\n",
       "      <td>P</td>\n",
       "      <td>buenas noches papus descansen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4787</td>\n",
       "      <td>@BenjaCandado eres una de las mejores personas...</td>\n",
       "      <td>P</td>\n",
       "      <td>mejores personas conocido vida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4790</td>\n",
       "      <td>Nos separan mas de 9KM y quieres JAJAJAJAJAJAJ...</td>\n",
       "      <td>P</td>\n",
       "      <td>separan km quieres jajajajajajajajajajajajajaj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4791</td>\n",
       "      <td>@CuadradoAndres @grazianopascale @adeladubra j...</td>\n",
       "      <td>P</td>\n",
       "      <td>jaja muchas gracias buena onda implicados</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4793</td>\n",
       "      <td>@Niaso01 @LuisSuarez9 @neymarjr El futbol es h...</td>\n",
       "      <td>P</td>\n",
       "      <td>futbol hermoso cdo juntan talentos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4794</td>\n",
       "      <td>#FelizDOMINGO que la paz de dios llene tu vida...</td>\n",
       "      <td>P</td>\n",
       "      <td>paz dios llene vida bello dia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1393 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                content sentiment  \\\n",
       "1     @SandraCauffman Hola Sandrita. No le habia des...         P   \n",
       "6     @doriamdiaz El de Halfon de Germinal se ve mor...         P   \n",
       "8     El amor es paciente, es bondadoso, no es envid...         P   \n",
       "9     El amanecer respirando o2 puro, es mas que un ...         P   \n",
       "14                   Buenas noches papus, que descansen         P   \n",
       "...                                                 ...       ...   \n",
       "4787  @BenjaCandado eres una de las mejores personas...         P   \n",
       "4790  Nos separan mas de 9KM y quieres JAJAJAJAJAJAJ...         P   \n",
       "4791  @CuadradoAndres @grazianopascale @adeladubra j...         P   \n",
       "4793  @Niaso01 @LuisSuarez9 @neymarjr El futbol es h...         P   \n",
       "4794  #FelizDOMINGO que la paz de dios llene tu vida...         P   \n",
       "\n",
       "                                          clean_content  \n",
       "1     hola sandrita deseado feliz dia madre tarde se...  \n",
       "6     halfon germinal mortal mary bang volando todav...  \n",
       "8     amor paciente bondadoso envidioso orgulloso eg...  \n",
       "9     amanecer respirando puro regalo frente mar pre...  \n",
       "14                        buenas noches papus descansen  \n",
       "...                                                 ...  \n",
       "4787                     mejores personas conocido vida  \n",
       "4790  separan km quieres jajajajajajajajajajajajajaj...  \n",
       "4791          jaja muchas gracias buena onda implicados  \n",
       "4793                 futbol hermoso cdo juntan talentos  \n",
       "4794                      paz dios llene vida bello dia  \n",
       "\n",
       "[1393 rows x 3 columns]"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_tweets = df_tweets_classified.query(\"sentiment == 'P'\")\n",
    "positive_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>clean_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Si andan haciendo eso mejor se quedaran callad...</td>\n",
       "      <td>N</td>\n",
       "      <td>andan haciendo mejor quedaran calladitas jaja ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Que pereza quiero choco banano</td>\n",
       "      <td>N</td>\n",
       "      <td>pereza quiero choco banano</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>@robertobrenes Bueno, no es tanto lo mayor com...</td>\n",
       "      <td>N</td>\n",
       "      <td>bueno mayor cuanto campo usted sos cartaguito ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>Acabo de ver una chamaca con un piercing de mo...</td>\n",
       "      <td>N</td>\n",
       "      <td>acabo ver chamaca piercing mota ombligo peor c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>Acabo de ver a una excompañera de trabajo y me...</td>\n",
       "      <td>N</td>\n",
       "      <td>acabo ver excompañera trabajo senti tentado sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4783</td>\n",
       "      <td>Queria un juguito y no puedo por el colorante</td>\n",
       "      <td>N</td>\n",
       "      <td>queria juguito puedo colorante</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4786</td>\n",
       "      <td>@NinetalesGD NO ME JODAS... que gran lastima</td>\n",
       "      <td>N</td>\n",
       "      <td>jodas gran lastima</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4789</td>\n",
       "      <td>Que lindo, los aliens eligieron visitar a Urug...</td>\n",
       "      <td>N</td>\n",
       "      <td>lindo aliens eligieron visitar uruguay proxima...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4797</td>\n",
       "      <td>A mi desayuno le hizo falta un alfajor podrida...</td>\n",
       "      <td>N</td>\n",
       "      <td>desayuno hizo falta alfajor podrida galletas a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4798</td>\n",
       "      <td>Viste cuando necesitas que alguien te escuche ...</td>\n",
       "      <td>N</td>\n",
       "      <td>viste necesitas alguien escuche alguien bueno</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1884 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                content sentiment  \\\n",
       "2     Si andan haciendo eso mejor se quedaran callad...         N   \n",
       "3                        Que pereza quiero choco banano         N   \n",
       "4     @robertobrenes Bueno, no es tanto lo mayor com...         N   \n",
       "10    Acabo de ver una chamaca con un piercing de mo...         N   \n",
       "12    Acabo de ver a una excompañera de trabajo y me...         N   \n",
       "...                                                 ...       ...   \n",
       "4783      Queria un juguito y no puedo por el colorante         N   \n",
       "4786       @NinetalesGD NO ME JODAS... que gran lastima         N   \n",
       "4789  Que lindo, los aliens eligieron visitar a Urug...         N   \n",
       "4797  A mi desayuno le hizo falta un alfajor podrida...         N   \n",
       "4798  Viste cuando necesitas que alguien te escuche ...         N   \n",
       "\n",
       "                                          clean_content  \n",
       "2     andan haciendo mejor quedaran calladitas jaja ...  \n",
       "3                            pereza quiero choco banano  \n",
       "4     bueno mayor cuanto campo usted sos cartaguito ...  \n",
       "10    acabo ver chamaca piercing mota ombligo peor c...  \n",
       "12    acabo ver excompañera trabajo senti tentado sa...  \n",
       "...                                                 ...  \n",
       "4783                     queria juguito puedo colorante  \n",
       "4786                                 jodas gran lastima  \n",
       "4789  lindo aliens eligieron visitar uruguay proxima...  \n",
       "4797  desayuno hizo falta alfajor podrida galletas a...  \n",
       "4798      viste necesitas alguien escuche alguien bueno  \n",
       "\n",
       "[1884 rows x 3 columns]"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_tweets = df_tweets_classified.query(\"sentiment == 'N'\")\n",
    "negative_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Tokenizing the Data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_token_tweets_list=[]\n",
    "neg_token_tweets_list=[]\n",
    "\n",
    "for token in positive_tweets['clean_content'].str.split():   \n",
    "    pos_token_tweets_list.append(token)\n",
    "\n",
    "for token in negative_tweets['clean_content'].str.split():          \n",
    "    neg_token_tweets_list.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gracias', 'dios', 'cerrando', 'broche', 'oro', 'dios', 'siempre', 'dando', 'mejor', 'hijos', 'semana', 'mejor', 'pasada'] \n",
      "\n",
      "['horario', 'semestre', 'tan', 'tan', 'feo', 'despedir', 'poca', 'vida', 'social', 'estreno', 'suicide', 'squad']\n"
     ]
    }
   ],
   "source": [
    "print(pos_token_tweets_list[9],'\\n')\n",
    "print(neg_token_tweets_list[9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Normalizing the Data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish_postagger = POS_Tag('C:\\\\Users\\\\gdlsajor\\\\AppData\\\\Roaming\\\\nltk_data\\\\models\\\\spanish.tagger', 'C:\\\\Users\\\\gdlsajor\\\\AppData\\\\Roaming\\\\nltk_data\\\\stanford-postagger.jar', encoding='utf8')\n",
    "java_path = \"C:/Program Files/Java/jre1.8.0_201/bin/java.exe\"\n",
    "os.environ['JAVAHOME'] = java_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(words):    \n",
    "    tokens = []\n",
    "    tagged_words = spanish_postagger.tag(words) ### Normalizing data            \n",
    "    #print (tagged_words) \n",
    "        \n",
    "    for (word, tag) in tagged_words:                      \n",
    "        if tag not in ['np00000','word','nc0n000','di0000','pr000000','vaip000','sp000','z0','i']:\n",
    "            #print(word+' '+tag)\n",
    "            tokens.append(word)\n",
    "    return tokens\n",
    "#https://stackoverflow.com/questions/14732465/nltk-tagging-spanish-words-using-a-corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_tweets_classified = pd.DataFrame(columns = columns)\n",
    "pos_tokens_normalized = []\n",
    "neg_tokens_normalized = []\n",
    "\n",
    "for words in pos_token_tweets_list:\n",
    "    #pos_tokens_normalized.append(normalization(words))\n",
    "\n",
    "for words in neg_token_tweets_list:\n",
    "    #neg_tokens_normalized.append(normalization(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esperando que sea un buen jueves por la noche\n",
      "['esperando', 'buen', 'noche'] \n",
      "\n",
      "Ya no tengo libro para leer ni serie para ver, mis noches se estan volviendo aburridas\n",
      "['libro', 'leer', 'serie', 'ver', 'noches', 'volviendo', 'aburridas']\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "x=130\n",
    "print(positive_tweets.content.iloc[x])\n",
    "print(pos_tokens_normalized[x],'\\n')\n",
    "\n",
    "print(negative_tweets.content.iloc[x])\n",
    "print(neg_tokens_normalized[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export lists to text files\n",
    "\n",
    "with open(r'data\\clean\\pos_tokens_normalized.txt', \"w\") as f:\n",
    "    for s in pos_tokens_normalized:\n",
    "        f.write(str(s) +\"\\n\")\n",
    "\n",
    "with open(r'data\\clean\\neg_tokens_normalized.txt', \"w\") as f:\n",
    "    for s in neg_tokens_normalized:\n",
    "        f.write(str(s) +\"\\n\")\n",
    "\n",
    "#pos_tokens_normalized.to_csv(r'data\\clean\\pos_tokens_normalized.csv', index = None, header=True)\n",
    "#neg_tokens_normalized.to_csv(r'data\\clean\\neg_tokens_normalized.csv', index = None, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['pereza', 'quiero', 'choco', 'banano']\""
      ]
     },
     "execution_count": 512,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import\n",
    "score=[]\n",
    "with open(r'data\\clean\\neg_tokens_normalized.txt', \"r\") as f:\n",
    "    for line in f:\n",
    "        score.append(str(line.strip()))\n",
    "score[1]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Determining Word Density</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_words(cleaned_tokens_list):\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        for token in tokens:\n",
    "            yield token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('feliz', 110), ('dia', 100), ('mejor', 97), ('hoy', 90), ('gracias', 77), ('bien', 75), ('buen', 62), ('buena', 62), ('dias', 62), ('asi', 60), ('siempre', 58), ('ser', 55), ('tan', 53), ('año', 51), ('ver', 48), ('quiero', 47), ('solo', 47), ('bonito', 47), ('lindo', 46), ('vida', 44)] \n",
      "\n",
      "[('triste', 90), ('solo', 83), ('quiero', 82), ('tan', 76), ('hoy', 75), ('asi', 73), ('dia', 70), ('ser', 68), ('mal', 67), ('ahora', 63), ('vida', 62), ('ver', 60), ('hacer', 56), ('voy', 56), ('puedo', 55), ('mejor', 53), ('año', 52), ('hace', 49), ('extraño', 48), ('cosas', 48)]\n"
     ]
    }
   ],
   "source": [
    "all_pos_words = get_all_words(pos_tokens_normalized)\n",
    "all_neg_words = get_all_words(neg_tokens_normalized)\n",
    "\n",
    "freq_dist_pos = FreqDist(all_pos_words)\n",
    "freq_dist_neg = FreqDist(all_neg_words)\n",
    "\n",
    "print(freq_dist_pos.most_common(20),'\\n')\n",
    "print(freq_dist_neg.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Preparing Data for the Model</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting Tokens to a Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets_for_model(cleaned_tokens_list):\n",
    "    for tweet_tokens in cleaned_tokens_list:\n",
    "        yield dict([token, True] for token in tweet_tokens)\n",
    "\n",
    "positive_tokens_for_model = get_tweets_for_model(pos_tokens_normalized)\n",
    "negative_tokens_for_model = get_tweets_for_model(neg_tokens_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Splitting the Dataset for Training and Testing the Model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "positive_dataset = [(tweet_dict, \"Positive\")\n",
    "                     for tweet_dict in positive_tokens_for_model]\n",
    "\n",
    "negative_dataset = [(tweet_dict, \"Negative\")\n",
    "                     for tweet_dict in negative_tokens_for_model]\n",
    "\n",
    "dataset = positive_dataset + negative_dataset\n",
    "\n",
    "random.shuffle(dataset)\n",
    "\n",
    "train_data = dataset[:2000]\n",
    "test_data = dataset[2000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Building and Testing the Model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.7392325763508223\n",
      "Most Informative Features\n",
      "                    peor = True           Negati : Positi =     13.7 : 1.0\n",
      "                  triste = True           Negati : Positi =     11.6 : 1.0\n",
      "                    dios = True           Positi : Negati =      9.4 : 1.0\n",
      "                    odio = True           Negati : Positi =      8.2 : 1.0\n",
      "                 encanta = True           Positi : Negati =      7.6 : 1.0\n",
      "                   buena = True           Positi : Negati =      7.3 : 1.0\n",
      "                 hermoso = True           Positi : Negati =      7.2 : 1.0\n",
      "                 gracias = True           Positi : Negati =      7.0 : 1.0\n",
      "               esperando = True           Positi : Negati =      6.7 : 1.0\n",
      "                  llegar = True           Positi : Negati =      6.7 : 1.0\n",
      "                   exito = True           Positi : Negati =      6.7 : 1.0\n",
      "                    pais = True           Negati : Positi =      6.2 : 1.0\n",
      "                  genial = True           Positi : Negati =      6.2 : 1.0\n",
      "                   feliz = True           Positi : Negati =      6.0 : 1.0\n",
      "                  bonito = True           Positi : Negati =      5.9 : 1.0\n",
      "                 alguien = True           Negati : Positi =      5.8 : 1.0\n",
      "              importante = True           Positi : Negati =      5.8 : 1.0\n",
      "                    rica = True           Positi : Negati =      5.8 : 1.0\n",
      "                  nuevos = True           Positi : Negati =      5.8 : 1.0\n",
      "                   pobre = True           Negati : Positi =      5.7 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "classifier = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n",
    "\n",
    "print(classifier.show_most_informative_features(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "#nltk.download('punkt')\n",
    "\n",
    "#custom_tweet = \"I ordered just once from TerribleCo, they screwed up, never used the app again.\"\n",
    "#custom_tweet = 'Congrats #SportStar on your 7th best goal from last season winning goal of the year :) #Baller #Topbin #oneofmanyworldies'\n",
    "#custom_tweet = 'Thank you for sending my baggage to CityX and flying me to CityY at the same time. Brilliant service. #thanksGenericAirline'\n",
    "\n",
    "custom_tweet = \"Bad day today\"\n",
    "\n",
    "custom_tokens = word_tokenize(custom_tweet)\n",
    "#custom_tokens = remove_noise(word_tokenize(custom_tweet))\n",
    "\n",
    "print(classifier.classify(dict([token, True] for token in custom_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensorflow\n",
    "# To save:\n",
    "import pickle\n",
    "f = open('sentiment_classifier.pickle', 'wb')\n",
    "pickle.dump(classifier, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load:\n",
    "import pickle\n",
    "f = open('sentiment_classifier.pickle', 'rb')\n",
    "classifier = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
