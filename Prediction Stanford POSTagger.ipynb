{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T19:43:52.006881Z",
     "start_time": "2020-05-02T19:43:52.004297Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T16:15:57.296564Z",
     "start_time": "2020-05-30T16:15:57.292197Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.tag.stanford import StanfordPOSTagger as POS_Tag \n",
    "# Download the tagger from the site: https://nlp.stanford.edu/software/tagger.shtml Extract the file:stanford-postagger.jar\n",
    "from nltk import FreqDist\n",
    "import random\n",
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read clean data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_twitts = pd.read_csv('data/clean/es_twitts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Tokenizing and Removing Noise from the Data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_noise(desc, stop_words = ()):        \n",
    "        \n",
    "    desc = re.sub('https?:\\/\\/.*[\\r\\n]*','', desc)    \n",
    "    desc = re.sub(\"[^A-ZÑa-zñ]+\",\" \", desc)\n",
    "    desc = desc.lower()\n",
    "    cleaned_tokens = []    \n",
    "    \n",
    "    for token in desc.split(): ## Tokenizing the Data        \n",
    "        if token not in stop_words:\n",
    "            #print(token)\n",
    "            cleaned_tokens.append(token)\n",
    "        #else:\n",
    "            #print('STOPWORDS:',token)\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gibra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load stopwords\n",
    "stop_words = stopwords.words('spanish')\n",
    "stop_words.extend(['mexico', 'coronavirusoutbreak', 'casos', 'covid', 'coronavirus', 'coronavid', 'coronaviruspandemic', 'pandemia', 'cuarentena', 'virus','cdmx','mx'])\n",
    "\n",
    "# TODO: Use pandas table \n",
    "tweets = es_twitts['text'].tolist()\n",
    "tweets_cleaned_tokens_list = []\n",
    "\n",
    "for tokens in tweets:\n",
    "    # print(tokens)\n",
    "    tweets_cleaned_tokens_list.append(remove_noise(tokens, stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Si empezaste a trabajar, necesitas dar de alta a tus beneficiarios ante el IMSS, ahora lo puedes hacer desde tu domicilio a traves de internet y evita filas | SanaDistancia\n",
      "QuedateEnCasa \n",
      "Coronavirus\n",
      "COVID19 MexicoUnido          \n",
      "\n",
      "https://t.co/zv3POwhVXe https://t.co/1VOKagjdOF\n",
      "\n",
      "['Si', 'empezaste', 'a', 'trabajar,', 'necesitas', 'dar', 'de', 'alta', 'a', 'tus', 'beneficiarios', 'ante', 'el', 'IMSS,', 'ahora', 'lo', 'puedes', 'hacer', 'desde', 'tu', 'domicilio', 'a', 'traves', 'de', 'internet', 'y', 'evita', 'filas', '|', 'SanaDistancia', 'QuedateEnCasa', 'Coronavirus', 'COVID19', 'MexicoUnido', 'https://t.co/zv3POwhVXe', 'https://t.co/1VOKagjdOF']\n",
      "\n",
      "['si', 'empezaste', 'trabajar', 'necesitas', 'dar', 'alta', 'beneficiarios', 'imss', 'ahora', 'puedes', 'hacer', 'domicilio', 'traves', 'internet', 'evita', 'filas', 'sanadistancia', 'quedateencasa', 'mexicounido']\n"
     ]
    }
   ],
   "source": [
    "x=2\n",
    "print(tweets[x])\n",
    "print('')\n",
    "print(tweets[x].split())\n",
    "print('')\n",
    "print(tweets_cleaned_tokens_list[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Normalizing data   from the Data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish_postagger = POS_Tag(\n",
    "    r'stanford-tagger\\models\\spanish-ud.tagger',\n",
    "    r'stanford-tagger\\stanford-postagger-4.0.0.jar',\n",
    "    encoding='utf8'\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(words,taggstarts):\n",
    "    nouns = []    \n",
    "    tagged_words = spanish_postagger.tag(words) ### Normalizing data            \n",
    "    print (tagged_words) \n",
    "    \n",
    "    for (word, tag) in tagged_words:      \n",
    "        print(word + ' ' + tag)        \n",
    "        if tag.startswith(taggstarts):\n",
    "            nouns.append(word)            \n",
    "    #print(nouns)    \n",
    "    return nouns\n",
    "\n",
    "#https://stackoverflow.com/questions/14732465/nltk-tagging-spanish-words-using-a-corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cualquier', 'DET'), ('enfermedad', 'NOUN'), ('respiratoria', 'ADJ'), ('automediques', 'ADJ'), ('prevencioncoronavirus', 'ADJ')]\n",
      "cualquier DET\n",
      "enfermedad NOUN\n",
      "respiratoria ADJ\n",
      "automediques ADJ\n",
      "prevencioncoronavirus ADJ\n",
      "[('cualquier', 'DET'), ('enfermedad', 'NOUN'), ('respiratoria', 'ADJ'), ('automediques', 'ADJ'), ('prevencioncoronavirus', 'ADJ')]\n",
      "cualquier DET\n",
      "enfermedad NOUN\n",
      "respiratoria ADJ\n",
      "automediques ADJ\n",
      "prevencioncoronavirus ADJ\n",
      "[('cualquier', 'DET'), ('enfermedad', 'NOUN'), ('respiratoria', 'ADJ'), ('automediques', 'ADJ'), ('prevencioncoronavirus', 'ADJ')]\n",
      "cualquier DET\n",
      "enfermedad NOUN\n",
      "respiratoria ADJ\n",
      "automediques ADJ\n",
      "prevencioncoronavirus ADJ\n",
      "[('atenci', 'NOUN'), ('n', 'NOUN'), ('terminal', 'NOUN'), ('nuevo', 'ADJ'), ('circo', 'NOUN'), ('implementan', 'VERB'), ('medidas', 'NOUN'), ('uso', 'NOUN'), ('mascarilla', 'VERB'), ('parte', 'NOUN'), ('usuaris', 'NOUN'), ('conductores', 'ADJ'), ('hacen', 'VERB'), ('vida', 'NOUN'), ('dichas', 'ADJ'), ('instalaciones', 'NOUN'), ('parte', 'NOUN'), ('esfuerzos', 'NOUN'), ('unificados', 'ADJ'), ('impedir', 'AUX'), ('propagacion', 'VERB'), ('marzo', 'NOUN'), ('nicolasmaduro', 'ADJ'), ('erikapsuv', 'ADJ')]\n",
      "atenci NOUN\n",
      "n NOUN\n",
      "terminal NOUN\n",
      "nuevo ADJ\n",
      "circo NOUN\n",
      "implementan VERB\n",
      "medidas NOUN\n",
      "uso NOUN\n",
      "mascarilla VERB\n",
      "parte NOUN\n",
      "usuaris NOUN\n",
      "conductores ADJ\n",
      "hacen VERB\n",
      "vida NOUN\n",
      "dichas ADJ\n",
      "instalaciones NOUN\n",
      "parte NOUN\n",
      "esfuerzos NOUN\n",
      "unificados ADJ\n",
      "impedir AUX\n",
      "propagacion VERB\n",
      "marzo NOUN\n",
      "nicolasmaduro ADJ\n",
      "erikapsuv ADJ\n",
      "[('atenci', 'NOUN'), ('n', 'NOUN'), ('terminal', 'NOUN'), ('nuevo', 'ADJ'), ('circo', 'NOUN'), ('implementan', 'VERB'), ('medidas', 'NOUN'), ('uso', 'NOUN'), ('mascarilla', 'VERB'), ('parte', 'NOUN'), ('usuaris', 'NOUN'), ('conductores', 'ADJ'), ('hacen', 'VERB'), ('vida', 'NOUN'), ('dichas', 'ADJ'), ('instalaciones', 'NOUN'), ('parte', 'NOUN'), ('esfuerzos', 'NOUN'), ('unificados', 'ADJ'), ('impedir', 'AUX'), ('propagacion', 'VERB'), ('marzo', 'NOUN'), ('nicolasmaduro', 'ADJ'), ('erikapsuv', 'ADJ')]\n",
      "atenci NOUN\n",
      "n NOUN\n",
      "terminal NOUN\n",
      "nuevo ADJ\n",
      "circo NOUN\n",
      "implementan VERB\n",
      "medidas NOUN\n",
      "uso NOUN\n",
      "mascarilla VERB\n",
      "parte NOUN\n",
      "usuaris NOUN\n",
      "conductores ADJ\n",
      "hacen VERB\n",
      "vida NOUN\n",
      "dichas ADJ\n",
      "instalaciones NOUN\n",
      "parte NOUN\n",
      "esfuerzos NOUN\n",
      "unificados ADJ\n",
      "impedir AUX\n",
      "propagacion VERB\n",
      "marzo NOUN\n",
      "nicolasmaduro ADJ\n",
      "erikapsuv ADJ\n",
      "[('atenci', 'NOUN'), ('n', 'NOUN'), ('terminal', 'NOUN'), ('nuevo', 'ADJ'), ('circo', 'NOUN'), ('implementan', 'VERB'), ('medidas', 'NOUN'), ('uso', 'NOUN'), ('mascarilla', 'VERB'), ('parte', 'NOUN'), ('usuaris', 'NOUN'), ('conductores', 'ADJ'), ('hacen', 'VERB'), ('vida', 'NOUN'), ('dichas', 'ADJ'), ('instalaciones', 'NOUN'), ('parte', 'NOUN'), ('esfuerzos', 'NOUN'), ('unificados', 'ADJ'), ('impedir', 'AUX'), ('propagacion', 'VERB'), ('marzo', 'NOUN'), ('nicolasmaduro', 'ADJ'), ('erikapsuv', 'ADJ')]\n",
      "atenci NOUN\n",
      "n NOUN\n",
      "terminal NOUN\n",
      "nuevo ADJ\n",
      "circo NOUN\n",
      "implementan VERB\n",
      "medidas NOUN\n",
      "uso NOUN\n",
      "mascarilla VERB\n",
      "parte NOUN\n",
      "usuaris NOUN\n",
      "conductores ADJ\n",
      "hacen VERB\n",
      "vida NOUN\n",
      "dichas ADJ\n",
      "instalaciones NOUN\n",
      "parte NOUN\n",
      "esfuerzos NOUN\n",
      "unificados ADJ\n",
      "impedir AUX\n",
      "propagacion VERB\n",
      "marzo NOUN\n",
      "nicolasmaduro ADJ\n",
      "erikapsuv ADJ\n"
     ]
    }
   ],
   "source": [
    "n_tweets_noun_tokens_list = []\n",
    "a_tweets_noun_tokens_list = []\n",
    "v_tweets_noun_tokens_list = []\n",
    "for words in tweets_cleaned_tokens_list[:2]:    \n",
    "    n_tweets_noun_tokens_list.append(normalization(words,'N'))\n",
    "    a_tweets_noun_tokens_list.append(normalization(words,'A'))\n",
    "    v_tweets_noun_tokens_list.append(normalization(words,'V'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['enfermedad'],\n",
       " ['atenci',\n",
       "  'n',\n",
       "  'terminal',\n",
       "  'circo',\n",
       "  'medidas',\n",
       "  'uso',\n",
       "  'parte',\n",
       "  'usuaris',\n",
       "  'vida',\n",
       "  'instalaciones',\n",
       "  'parte',\n",
       "  'esfuerzos',\n",
       "  'marzo']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_tweets_noun_tokens_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['enfermedad']\n",
      "\n",
      "['enfermedad']\n"
     ]
    }
   ],
   "source": [
    "x=0\n",
    "print(n_tweets_noun_tokens_list[x])\n",
    "print ('')\n",
    "print(n_tweets_noun_tokens_list[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Determining Word Density</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_words(cleaned_tokens_list):\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        for token in tokens:\n",
    "            yield token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('parte', 2), ('enfermedad', 1), ('atenci', 1), ('n', 1), ('terminal', 1), ('circo', 1), ('medidas', 1), ('uso', 1), ('usuaris', 1), ('vida', 1), ('instalaciones', 1), ('esfuerzos', 1), ('marzo', 1)]\n"
     ]
    }
   ],
   "source": [
    "#all_pos_words = get_all_words(tweets_cleaned_tokens_list)\n",
    "all_pos_words = get_all_words(n_tweets_noun_tokens_list)\n",
    "freq_dist_pos = FreqDist(all_pos_words)\n",
    "print(freq_dist_pos.most_common(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Preparing Data for the Model</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting Tokens to a Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets_for_model(cleaned_tokens_list):\n",
    "    for tweet_tokens in cleaned_tokens_list:\n",
    "        yield dict([token, True] for token in tweet_tokens)\n",
    "\n",
    "positive_tokens_for_model = get_tweets_for_model(n_tweets_noun_tokens_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Splitting the Dataset for Training and Testing the Model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_dataset = [(tweet_dict, \"Positive\")\n",
    "                     for tweet_dict in positive_tokens_for_model]\n",
    "\n",
    "dataset = positive_dataset\n",
    "\n",
    "random.shuffle(dataset)\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# train_set, test_set = train_test_split(dataset, test_size=0.30, random_state=42)\n",
    "\n",
    "train_data = dataset[:7000]\n",
    "test_data = dataset[7000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Building and Testing the Model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0\n",
      "Most Informative Features\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "classifier = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n",
    "\n",
    "print(classifier.show_most_informative_features(10))"
   ]
  }
 ],
 "metadata": {
  "hide_input": true,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "209.8px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 531.8,
   "position": {
    "height": "553.4px",
    "left": "930px",
    "right": "20px",
    "top": "120px",
    "width": "587.8px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
