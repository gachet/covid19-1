{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On this Data Clean process for tweets we will apply a Filter, remove noise data like accents, urls, hashtags, among others. The output is an csv with data clean to be used in the sentiment analysis model and topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from nltk.corpus import stopwords\n",
    "import re, string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## At glace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all Tweets file paths\n",
    "raw_data_path = r'data/raw/*Tweets*.csv'\n",
    "csv_files = glob(raw_data_path)\n",
    "csv_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial dataset files are not available on the path mentioned due to space limitation on GitHub. Once you have this project downloaded in your local machine download such as csv files from below link and place them at \"data/raw/\"\n",
    "\n",
    "https://www.kaggle.com/smid80/coronavirus-covid19-tweets-early-april"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_columns = ['created_at','text', 'retweet_count', 'country_code', 'followers_count', 'lang', 'screen_name']\n",
    "data_frames = []\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file, encoding='utf-8', usecols=use_columns)\n",
    "    data_frames.append(df)\n",
    "df = pd.concat(data_frames)\n",
    "del data_frames\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter data tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by tweets form Mexico only\n",
    "mx_twitts = df.query(\"country_code == 'MX'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Noise from the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .words() method to get a list of stop words in Spanish\n",
    "stop_words = stopwords.words('spanish')\n",
    "# stopwords is extended by adding unncessary words\n",
    "stop_words.extend(['coronavirus','covid19', 'covid_19','pandemia','cuarentena','covid','mexico','covid2019','covid19mx','cdmx','covidー19','coronaviru','coronavid19','coronavirusmexico','quedateencasa','tiempo','solo','aqui','caso','casa','pais','casos','casas','ahora','gente','persona','mundo','momento','parte','dice','toda','hacer','hace','ssalud','yomequedoencasa','mexicano','cosa','pues','video','dias','puede','personas','mismo','tema','importante','tiempos','medida','nuevo'])\n",
    "\n",
    "# word accents from stopword list are removed for the clean process\n",
    "stop_words = [re.sub('á','a', i) for i in stop_words]\n",
    "stop_words = [re.sub('é','e', i) for i in stop_words]\n",
    "stop_words = [re.sub('í','i', i) for i in stop_words]\n",
    "stop_words = [re.sub('ó','o', i) for i in stop_words]\n",
    "stop_words = [re.sub('ú','u', i) for i in stop_words]\n",
    "\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove output warning message\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# Transform all words in lower case in a new column called \"clean_text\"\n",
    "mx_twitts[\"clean_text\"] = mx_twitts[\"text\"].str.lower()\n",
    "\n",
    "# word accents from tweet list are removed for the clean process\n",
    "mx_twitts.replace('á','a', regex=True, inplace=True)\n",
    "mx_twitts.replace('é','e', regex=True, inplace=True)\n",
    "mx_twitts.replace('í','i', regex=True, inplace=True)\n",
    "mx_twitts.replace('ó','o', regex=True, inplace=True)\n",
    "mx_twitts.replace('ú','u', regex=True, inplace=True)\n",
    "\n",
    "# remove from words urls, tags, hastashs, special characters and words which contain 1 to 3 letters.  \n",
    "mx_twitts['clean_text'].replace('http\\S+','',regex=True, inplace = True)\n",
    "mx_twitts['clean_text'] = mx_twitts['clean_text'].map(lambda x: re.sub(r'@\\S+', ' ', x))\n",
    "mx_twitts['clean_text'] = mx_twitts['clean_text'].map(lambda x: re.sub(r'#\\S+', ' ', x))\n",
    "mx_twitts['clean_text'] = mx_twitts['clean_text'].map(lambda x: re.sub(r'[^a-zñ]+', ' ', x))\n",
    "mx_twitts['clean_text'] = mx_twitts['clean_text'].map(lambda x: re.sub(r'\\b\\w{1,3}\\b', ' ', x))\n",
    "\n",
    "# remove stopwords \n",
    "mx_twitts['clean_text']=mx_twitts['clean_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "\n",
    "# Set \"NaN\" to create a NaN value for those empty fields after cleaning process\n",
    "mx_twitts.replace(\"\", float(\"NaN\"), inplace=True)\n",
    "\n",
    "# drop all rows that contain NaN under text clean column.\n",
    "mx_twitts.dropna(subset = [\"clean_text\"], inplace=True)\n",
    "\n",
    "mx_twitts[['text','clean_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (\" \").join(mx_twitts.clean_text.tolist())\n",
    "\n",
    "# Create and generate a word cloud image:\n",
    "wordcloud = WordCloud().generate(text)\n",
    "\n",
    "# Display the generated image:\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export clean es twitts to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exported clean tweets will be used in the sentiment analysis model and topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Export csv twitts in Spanish\n",
    "mx_twitts.to_csv(r'data\\clean\\mx_twitts.csv', index = None, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
